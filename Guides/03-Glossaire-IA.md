# üìö Glossaire IA & LLM

## A

**Agent**
Programme autonome utilisant un LLM pour prendre des d√©cisions et ex√©cuter des actions.

**AGI (Artificial General Intelligence)**
Intelligence artificielle capable de r√©aliser n'importe quelle t√¢che intellectuelle humaine. N'existe pas encore.

**Agentic AI**
Approche o√π l'IA agit de mani√®re autonome, planifie et ex√©cute des t√¢ches complexes.

**API (Application Programming Interface)**
Interface permettant d'interagir avec un LLM de mani√®re programmatique.

---

## B

**Backpropagation**
Technique d'entra√Ænement des r√©seaux de neurones par propagation de l'erreur en arri√®re.

**Benchmark**
Test standardis√© pour √©valuer les performances d'un mod√®le.

**BERT**
Mod√®le de langage de Google, pr√©curseur des LLMs modernes.

---

## C

**Chain-of-Thought (CoT)**
Technique de prompting qui demande au mod√®le de raisonner √©tape par √©tape.

**Chunking**
D√©coupage de documents en morceaux plus petits pour le traitement.

**Completion**
R√©ponse g√©n√©r√©e par un LLM √† partir d'un prompt.

**Context Window**
Nombre maximum de tokens qu'un mod√®le peut traiter en une fois (ex: 128K pour GPT-4).

---

## D

**Dataset**
Ensemble de donn√©es utilis√© pour entra√Æner un mod√®le.

**Deployment**
Mise en production d'un mod√®le.

---

## E

**Embedding**
Repr√©sentation num√©rique (vecteur) d'un texte, permettant de mesurer la similarit√©.

**Encoder/Decoder**
Architecture de transformers. Encoder = comprend, Decoder = g√©n√®re.

---

## F

**Few-Shot**
Technique de prompting utilisant quelques exemples pour guider le mod√®le.

**Fine-tuning**
R√©entra√Ænement d'un mod√®le sur des donn√©es sp√©cifiques.

**Foundation Model**
Mod√®le de base pr√©-entra√Æn√© (GPT, Claude, etc.).

---

## G

**GPU**
Processeur graphique utilis√© pour l'entra√Ænement et l'inf√©rence.

**Grounding**
Ancrage des r√©ponses dans des faits v√©rifiables (RAG).

---

## H

**Hallucination**
Quand le mod√®le g√©n√®re des informations fausses avec confiance.

**Hyperparameters**
Param√®tres de configuration (temp√©rature, top-p, etc.).

---

## I

**Inf√©rence**
Utilisation du mod√®le pour g√©n√©rer une r√©ponse (vs entra√Ænement).

**In-Context Learning**
Apprentissage via le prompt lui-m√™me, sans r√©entra√Ænement.

---

## L

**LLM (Large Language Model)**
Mod√®le de langage massif entra√Æn√© sur des milliards de tokens.

**LoRA**
Technique de fine-tuning efficace (Low-Rank Adaptation).

---

## M

**MLOps**
Pratiques pour d√©ployer et maintenir des mod√®les ML en production.

**Multimodal**
Mod√®le capable de traiter plusieurs types de donn√©es (texte, image, audio).

---

## O

**Orchestration**
Coordination de plusieurs agents ou mod√®les.

**Output**
R√©ponse g√©n√©r√©e par le mod√®le.

---

## P

**Prompt**
Instruction donn√©e au mod√®le pour obtenir une r√©ponse.

**Prompt Engineering**
Art de formuler des prompts efficaces.

---

## R

**RAG (Retrieval-Augmented Generation)**
Technique combinant recherche documentaire et g√©n√©ration.

**RLHF (Reinforcement Learning from Human Feedback)**
M√©thode d'entra√Ænement utilisant les retours humains.

---

## S

**Self-Consistency**
Technique g√©n√©rant plusieurs r√©ponses et votant pour la meilleure.

**System Prompt**
Prompt d√©finissant le comportement global du mod√®le.

---

## T

**Temperature**
Param√®tre contr√¥lant la cr√©ativit√© (0 = d√©terministe, 1 = cr√©atif).

**Token**
Unit√© de texte (‚âà 4 caract√®res en anglais, ‚âà 3 en fran√ßais).

**Top-p (Nucleus Sampling)**
Param√®tre limitant les choix de tokens aux plus probables.

**Transformer**
Architecture de r√©seau de neurones √† la base des LLMs.

---

## V

**Vector Database**
Base de donn√©es stockant des embeddings (Pinecone, ChromaDB, etc.).

**Vision Model**
Mod√®le capable de comprendre les images.

---

## Z

**Zero-Shot**
Demander au mod√®le de faire une t√¢che sans exemples.
