# üîÆ 04. World Models : MuZero & Dreamer (Model-Based RL)

## 1. La Limite du Model-Free
DQN et PPO apprennent par essai-erreur. Il leur faut des milliards de frames pour apprendre.
Un humain apprend vite car il a un **Mod√®le du Monde** dans sa t√™te ("Si je l√¢che ce verre, il va tomber"). Il peut **planifier**.

Le **Model-Based RL** cherche √† apprendre ce mod√®le : $s_{t+1} = f(s_t, a_t)$.

---

## 2. MuZero (2019) : Apprendre sans d√©coder les pixels

AlphaGo connaissait les r√®gles du Go. MuZero ne les conna√Æt pas.
L'astuce g√©niale de MuZero : Il ne pr√©dit pas les pixels futurs (trop dur), il pr√©dit l'√©tat futur dans un **Espace Latent** (Cach√©).

### Les 3 Fonctions Cl√©s
1.  **Representation** $h(o)$ : Transforme l'observation (pixels) en √©tat cach√© $s^0$.
2.  **Dynamics** $g(s, a)$ : Pr√©dit l'√©tat futur $s^{k+1}$ et la r√©compense $r^k$. (C'est le simulateur mental).
3.  **Prediction** $f(s)$ : Pr√©dit la politique $\pi$ et la valeur $v$.

### Monte Carlo Tree Search (MCTS)
MuZero utilise son "Simulateur Mental" (Dynamics) pour explorer des futurs possibles dans sa t√™te *avant* de jouer. C'est le **System 2** (R√©flexion) appliqu√© au RL.

---

## 3. DreamerV3 (2023) : Mastering Diverse Domains

Dreamer pousse le concept encore plus loin. L'agent apprend enti√®rement dans son r√™ve (World Model) et n'interagit avec le monde r√©el que pour collecter des donn√©es.

### L'Architecture
*   **World Model** : Apprend √† pr√©dire le futur (Sequence Modeling).
*   **Actor-Critic** : S'entra√Æne sur les trajectoires imagin√©es par le World Model.

### La R√©volution "Symlog"
Le probl√®me du RL est l'√©chelle des r√©compenses. (Pong = +1, Mario = +1000).
DreamerV3 introduit la transformation $symlog(x) = sign(x) \ln(|x| + 1)$.
Cela √©crase les diff√©rences d'√©chelle. L'agent peut apprendre √† jouer √† Minecraft (+ diamants) et au Go (+1 victoire) avec les **m√™mes hyperparam√®tres**.

---

## 4. Pourquoi c'est le futur ? (Jepta, Sora)

Les mod√®les g√©n√©ratifs vid√©o (Sora) sont des World Models. Ils comprennent la physique.
La convergence est l√† :
*   **GenAI** : Apprend $P(Video)$. (Regarder le monde).
*   **Model-Based RL** : Apprend $P(Video | Action)$. (Agir sur le monde).

Le prochain "GPT-5" sera probablement un agent Model-Based capable de planifier dans un espace latent avant de r√©pondre.
