# ü§ñ 03. DQN, A2C, PPO : Les Algorithmes qui ont chang√© l'IA

## 1. Introduction : Model-Free vs Model-Based

Ici on parle de **Model-Free RL**. L'agent ne conna√Æt pas les r√®gles du jeu (il ne sait pas que la gravit√© fait tomber la pomme). Il apprend juste en regardant les pixels.

---

## 2. Deep Q-Network (DQN) - 2013 (DeepMind)

C'est l'algo qui a permis √† l'IA de jouer √† Atari.

### Le Probl√®me du Q-Learning Tabulaire
Dans un jeu simple, on stocke $Q(s,a)$ dans un tableau Excel g√©ant.
Mais une image Atari a $256^{210 \times 160}$ √©tats. Tableau impossible.

### L'Approche Deep
On remplace le tableau par un **R√©seau de Neurones** $Q_\theta(s, a)$.
Le r√©seau prend les pixels en entr√©e et sort la valeur de chaque action.

### L'Innovation : Experience Replay
Le RL est instable car les donn√©es sont corr√©l√©es (l'image √† $t$ ressemble √† $t+1$).
DQN stocke les transitions $(s, a, r, s')$ dans un "Buffer" et pioche dedans au hasard.
$$ L_i(\theta_i) = \mathbb{E} [(r + \gamma \max_{a'} Q(s', a'; \theta_{i-1}) - Q(s, a; \theta_i))^2] $$
*(On minimise la diff√©rence entre notre pr√©diction et la cible Bellman).*

---

## 3. Actor-Critic (A2C/A3C)

S√©parer le cerveau en deux :
1.  **Actor** $\pi_\theta(s)$ : D√©cide quelle action prendre (Le Joueur).
2.  **Critic** $V_w(s)$ : Juge si l'√©tat est bon (Le Coach).

Le Critic calcule l'**Advantage** $A(s,a)$ : "Est-ce que cette action √©tait mieux que la moyenne ?"
$$ A(s, a) = Q(s, a) - V(s) $$
L'Actor apprend dans la direction du gradient seulement si $A(s, a) > 0$.

---

## 4. Proximal Policy Optimization (PPO) - 2017 (OpenAI)

C'est l'algorithme "State of the Art" par d√©faut. Il a servi √† entra√Æner **GPT-3 via RLHF**.

### Le Probl√®me
En RL, si on change trop brutalement les poids du r√©seau, l'agent "tombe de la falaise" (Policy Collapse) et ne retrouve jamais le chemin.

### La Solution : Clipped Objective
PPO force la mise √† jour √† rester dans une petite zone de confiance ("Trust Region").
On compare la nouvelle politique $\pi$ et l'ancienne $\pi_{old}$ via un ratio $r(\theta)$.
Si $r(\theta)$ s'√©loigne trop de 1 (ex: > 1.2), on coupe le gradient (Clip).

$$ L^{CLIP}(\theta) = \mathbb{E} [ \min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t) ] $$

*   $\hat{A}_t$ : Avantage (Est-ce que l'action √©tait bonne ?)
*   $\epsilon$ : Hyperparam√®tre (souvent 0.2). On emp√™che le mod√®le de changer de plus de 20% √† la fois.

### Impact sur ChatGPT
PPO est stable et facile √† tuner. C'est pourquoi OpenAI l'a choisi pour aligner GPT :
1.  GPT g√©n√®re une phrase.
2.  Reward Model (entra√Æn√© par humains) donne un score.
3.  PPO met √† jour GPT pour maximiser ce score sans d√©truire ses capacit√©s linguistiques (gr√¢ce au Clip).
