# üéÆ 02. Reinforcement Learning (RL) : Les Bases Math√©matiques

![Boucle Apprentissage Renforcement](..\assets\images\rl-loop.png)

## 1. Le Paradigme Fondamental (MDP)

Le RL n'est pas de l'apprentissage supervis√© (Input -> Label). C'est de l'apprentissage par l'essai-erreur.
On mod√©lise le probl√®me comme un **Processus de D√©cision Markovien (MDP)** : $(S, A, P, R, \gamma)$.

*   **S (State)** : √âtat du monde (ex: pixels de l'√©cran Pong).
*   **A (Action)** : Ce que l'agent peut faire (ex: Haut, Bas).
*   **P (Transition)** : $P(s' | s, a)$ Probabilit√© d'arriver en $s'$ si je fais $a$ dans $s$.
*   **R (Reward)** : $R(s, a)$ R√©compense imm√©diate (ex: +1 si la balle passe, -1 si je perds).
*   **$\gamma$ (Gamma - Discount Factor)** : Pr√©f√©rence pour le pr√©sent. Si $\gamma=0$, l'agent ne voit que l'instant t. Si $\gamma=1$, il pense infini.

```mermaid
graph LR
    Agent -- Action (a) --> Environnement
    Environnement -- Reward (r) --> Agent
    Environnement -- Next State (s') --> Agent
```

---

## 2. L'Objectif : Maximiser le "Return"

L'agent ne veut pas juste le Reward imm√©diat $r_t$. Il veut la somme des rewards futurs $G_t$.
$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} $$

L'objectif est de trouver une **Politique $\pi(a|s)$** (Probabilit√© de jouer $a$ dans l'√©tat $s$) qui maximise l'esp√©rance de $G_t$.
$$ J(\pi) = \mathbb{E}_{\pi}[G_0] $$

---

## 3. Value Function & Q-Function

Comment l'agent sait-il si l'√©tat $s$ est "bon" ?
Il a besoin d'√©valuer la valeur des √©tats.

### State-Value Function $V(s)$
"Si je suis ici, combien de points je vais gagner en moyenne d'ici la fin ?"
$$ V_{\pi}(s) = \mathbb{E}_{\pi} [G_t | S_t = s] $$

### Action-Value Function $Q(s, a)$
"Si je suis ici et que je fais l'action $a$, combien je vais gagner ?"
C'est la fonction la plus importante en Deep RL.
$$ Q_{\pi}(s, a) = \mathbb{E}_{\pi} [G_t | S_t = s, A_t = a] $$

---

## 4. L'√âquation de Bellman (Le C≈ìur du RL)

C'est l'√©quation r√©cursive qui permet de "r√©soudre" le RL. Elle dit que la valeur d'un √©tat est √©gale √† la r√©compense imm√©diate + la valeur de l'√©tat suivant actualis√©e.

$$ V(s) = \max_a (R(s,a) + \gamma V(s')) $$

Cette √©quation permet √† l'information de "remonter" du futur vers le pr√©sent. C'est la base de la Programmation Dynamique.

---

## 5. Exploration vs Exploitation

Le dilemme √©ternel.
*   **Exploitation** : Je joue l'action que je *pense* √™tre la meilleure (Max Q).
*   **Exploration** : Je joue une action al√©atoire pour voir si elle n'est pas mieux.

### Epsilon-Greedy ($\epsilon$-greedy)
*   Avec probabilit√© $1-\epsilon$ : Jouer $\max Q(s,a)$.
*   Avec probabilit√© $\epsilon$ : Jouer $Random$.
On commence avec $\epsilon=1$ (Exploration totale) et on diminue vers 0.01.

---

## Conclusion
Le RL est puissant car il ne demande pas de donn√©es √©tiquet√©es.
Cependant, il est instable et demande des millions d'interactions ("Sample Inefficiency").
C'est pourquoi on le combine souvent avec le Deep Learning (DQN, PPO) pour approximer les fonctions $V$ et $Q$ quand l'espace des √©tats est trop grand (ex: pixels d'une image).
