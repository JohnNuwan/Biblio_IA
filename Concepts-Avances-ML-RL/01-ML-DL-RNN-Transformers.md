# üß† 01. De ML √† Transformers : L'√âvolution Math√©matique de la "Cognition"

## 1. Machine Learning vs Deep Learning : Le "Feature Engineering"

La diff√©rence fondamentale r√©side dans la repr√©sentation de la donn√©e.

### Machine Learning Classique (SVM, Random Forest)
L'humain doit extraire les caract√©ristiques (features) manuellement.
*   **Input** : Image de chat.
*   **Features Manual** : `a_des_oreilles_pointues=1`, `couleur=roux`.
*   **Model** : $y = w \cdot x + b$ (Le mod√®le p√®se les features).

### Deep Learning (R√©seaux de Neurones)
Le r√©seau apprend ses propres features (Representation Learning).
Les premi√®res couches apprennent les bords, les suivantes les formes, les derni√®res les objets.

---

## 2. Le Neurone Artificiel (Perceptron)

L'unit√© de base imitant le neurone biologique.

### Formule Math√©matique
$$ z = \sum (w_i \cdot x_i) + b $$
$$ a = \sigma(z) $$

*   $x_i$ : Inputs (donn√©es).
*   $w_i$ : Poids (Force de la connexion synaptique).
*   $b$ : Biais (Seuil d'activation).
*   $\sigma$ : Fonction d'activation (ReLU, Sigmoid, Tanh). C'est elle qui introduit la **non-lin√©arit√©**. Sans $\sigma$, un r√©seau profond n'est qu'une r√©gression lin√©aire g√©ante.

### Backpropagation (R√©tropropagation du Gradient)
C'est l'algorithme qui permet d'apprendre. On calcule l'erreur √† la fin et on remonte pour corriger les poids.
$$ \frac{\partial L}{\partial w} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial w} $$
*(R√®gle de la cha√Æne)*

---

## 3. L'Impasse des RNN & LSTM (1990-2015)

Pour traiter du texte (s√©quence), on utilisait des **Recurrent Neural Networks (RNN)**.
Le neurone $t$ prend en entr√©e le mot $x_t$ ET l'√©tat cach√© du neurone pr√©c√©dent $h_{t-1}$.

### Le Probl√®me : Vanishing Gradient
Sur une longue phrase, le gradient tend vers 0 lors de la r√©tropropagation. Le d√©but de la phrase est oubli√©.
$$ |W| < 1 \implies W^{100} \approx 0 $$

### La Solution Partielle : LSTM (Long Short-Term Memory)
Introduction de "Portes" (Gates) physiques pour laisser passer l'information.
*   Input Gate : Quoi Stocker ?
*   Forget Gate : Quoi oublier ?
*   Output Gate : Quoi dire ?

**Inconv√©nient Majeur** : C'est s√©quentiel. On ne peut pas calculer le mot 10 tant qu'on n'a pas calcul√© le mot 9. **Impossible de parall√©liser sur GPU.**

---

## 4. La R√©volution Transformer (2017) : "Attention is All You Need"

Google Brain choque le monde en supprimant la r√©currence. Plus de $t-1$. Tout est calcul√© en parall√®le.

### M√©canisme de Self-Attention (L'√©quation cl√©)
L'attention permet √† chaque mot de "regarder" tous les autres mots de la phrase pour comprendre le contexte.

$$ Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V $$

*   **Q (Query)** : Ce que je cherche (ex: le pronom "Il").
*   **K (Key)** : L'√©tiquette des autres mots (ex: "Sujet", "Verbe").
*   **V (Value)** : Le contenu des mots.

Si $Q$ ("Il") matche avec $K$ ("Jean"), le score est √©lev√©, et on r√©cup√®re beaucoup de $V$ ("Jean"). Le pronom "Il" absorbe le sens de "Jean".

```mermaid
graph TD
    Input[Input: "Jean mange la pomme"] --> Embedding
    Embedding --> PositionalEncoding[Positional Encoding]
    PositionalEncoding --> MultiHeadAttention
    MultiHeadAttention --> AddNorm1[Add & Norm]
    AddNorm1 --> FeedForward[Feed Forward Network]
    FeedForward --> AddNorm2[Add & Norm]
    AddNorm2 --> Output
    
    subgraph Attention Mechanism
    Q[Query]
    K[Key]
    V[Value]
    MatMul[MatMul Q*K] --> Scale
    Scale --> Softmax
    Softmax --> MatMul2[MatMul * V]
    end
```

### Pourquoi c'est une r√©volution ?
1.  **Parall√©lisation Infinie** : On peut entra√Æner sur 10.000 GPUs en m√™me temps.
2.  **Contexte Global** : Chaque mot "voit" toute la phrase instantan√©ment, pas de perte de m√©moire √† long terme.

C'est cette architecture qui a permis GPT-1, 2, 3, 4.
Nous sommes pass√©s d'une approche **S√©quentielle (Lente, Amn√©sique)** √† une approche **Attentionnelle (Rapide, Omnisciente)**.
