# 01 - Les R√©seaux de Neurones Composants de Base

![Neural Network Schematic](..\assets\images\neural-network.png)

Bienvenue sous, capot. Ici, on quitte les statistiques classiques pour entrer dans le **biomim√©tisme math√©matique** qui a permis l'√©mergence de ChatGPT.

---

## 1. Le Neurone Artificiel (Le Perceptron)

C'est l'unit√© atomique.

### üìú Origine & Histoire
Invent√© par **Frank Rosenblatt** en **1957** au Cornell Aeronautical Laboratory. C'√©tait une machine physique (le Mark I Perceptron), c√¢bl√©e manuellement !
L'id√©e √©tait de reproduire le fonctionnement d'un neurone biologique : recevoir des signaux √©lectriques (inputs), les sommer, et "tirer" (fire) un signal de sortie si un seuil est d√©pass√©.

### üßÆ La Math√©matique du Neurone
Un neurone fait une op√©ration tr√®s simple. Il re√ßoit des entr√©es $x$, chacune multipli√©e par un poids $w$ (l'importance de l'entr√©e), et ajoute un biais $b$.

$$z = \sum (w_i \cdot x_i) + b$$
$$z = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b$$

Puis, il passe ce r√©sultat $z$ dans une **Fonction d'Activation** $\sigma(z)$ pour d√©cider de la sortie $a$.
$$a = \sigma(z)$$

---

## 2. Les Fonctions d'Activation (Donner vie √† la non-lin√©arit√©)

Sans elles, un r√©seau de neurones (m√™me immense) ne serait qu'une grosse r√©gression lin√©aire. Elles introduisent la **non-lin√©arit√©**, permettant d'apprendre des courbes complexes.

### üîπ Sigmo√Øde (La vieille √©cole)
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$
*   √âcrase tout entre 0 et 1.
*   **Probl√®me** : "Vanishing Gradient" (l'apprentissage s'arr√™te si les valeurs sont trop grandes). Peu utilis√©e aujourd'hui dans les couches cach√©es.

### üîπ ReLU (Rectified Linear Unit) - Le standard actuel
$$f(x) = \max(0, x)$$
*   Si c'est n√©gatif, √ßa vaut 0 (neurone √©teint).
*   Si c'est positif, on garde la valeur.
*   **Pourquoi √ßa marche ?** : C'est extr√™mement rapide √† calculer et √ßa r√©sout le probl√®me du gradient qui dispara√Æt. C'est le moteur des LLMs modernes.

---

## 3. Le R√©seau (Multi-Layer Perceptron - MLP)

On empile ces neurones en couches :
1.  **Input Layer** : Les donn√©es brutes (les pixels d'une image, les mots d'une phrase).
2.  **Hidden Layers** : C'est l√† que la "magie" op√®re. Chaque couche d√©tecte des motifs de plus en plus complexes (bords -> formes -> yeux -> visages).
3.  **Output Layer** : La d√©cision finale (Chat/Chien).

---

## 4. Comment √ßa apprend ? (Backpropagation)

C'est LA r√©volution qui a tout d√©bloqu√© dans les ann√©es 80 (popularis√©e par **Hinton, LeCun, Bengio**).

### Le cycle d'apprentissage en 4 √©tapes :
1.  **Forward Pass (Pr√©diction)** : On fait passer les donn√©es dans le r√©seau. Le r√©seau dit "C'est un Chat (80%)".
2.  **Calcul de l'Erreur (Loss)** : On compare avec la r√©alit√© (C'√©tait un Chien). L'erreur est grande.
3.  **Backward Pass (Backpropagation)** : C'est l'√©tape cruciale. On remonte le r√©seau **√† l'envers** (de la sortie vers l'entr√©e) pour trouver *qui* est responsable de l'erreur.
    *   "Neurone A, tu as trop cri√© 'Chat', baisse un peu ton poids."
    *   "Neurone B, tu √©tais √©teint alors qu'il fallait t'allumer, augmente ton poids."
    *   Math√©matiquement, on utilise la **R√®gle de la Cha√Æne (Chain Rule)** pour calculer le gradient : $\frac{\partial Loss}{\partial w}$.
4.  **Mise √† jour (Optimizer)** : On modifie l√©g√®rement tous les poids ($w$) pour r√©duire l'erreur (via la Descente de Gradient).

### üè¢ Cas d'Usage
Avant les Transformers (2017), ces r√©seaux (CNN, RNN) ont r√©volutionn√© :
*   **La Poste** : Lecture automatique des codes postaux manuscrits (Yann LeCun, 1989).
*   **Siri / Alexa** : Reconnaissance vocale (RNN / LSTM).
*   **Facebook** : Tagging automatique des visages sur les photos.
