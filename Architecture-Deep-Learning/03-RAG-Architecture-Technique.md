# 03 - Architecture Technique du RAG (Retrieval Augmented Generation)

![Sch√©ma RAG](..\assets\images\rag-architecture.png)

Le RAG n'est pas juste "un prompt avec du contexte". C'est une architecture syst√®me compl√®te. Voici comment on construit un RAG de production.

---

## 1. Pourquoi le RAG ? (Le Probl√®me de l'Hallucination)
Les LLM sont fig√©s dans le temps (leur date de fin d'entra√Ænement).
*   **Probl√®me** : Demandez √† GPT-4 "Quel est le solde de mon compte aujourd'hui ?". Il ne sait pas.
*   **Solution** : On injecte la connaissance fra√Æche dans le prompt *avant* que le mod√®le ne r√©ponde.

---

## 2. Le Pipeline d'Ingestion (ETL for AI)

Avant de pouvoir poser des questions, il faut pr√©parer les donn√©es. C'est 80% du travail.

### A. Loading (Chargement)
Extraire le texte de sources vari√©es : PDF, Docx, HTML, SQL.
*   *D√©fi* : Les PDF sont terribles (tableaux, colonnes, headers). Utiliser des outils comme `Unstructured` ou `PyMuPDF`.

### B. Chunking (D√©coupage) ‚úÇÔ∏è
On ne peut pas envoyer un livre entier (limite de Context Window). Il faut d√©couper.
**Strat√©gies de Chunking :**
1.  **Fixed Size** : "Coupe tous les 500 caract√®res". (Brutal, risque de couper une phrase en deux).
2.  **Recursive Character Splitter** : "Essaie de couper aux paragraphes, sinon aux phrases, sinon aux mots". (Standard LangChain).
3.  **Semantic Chunking** : "Coupe quand le sujet change" (en utilisant les embeddings pour voir les sauts de distance s√©mantique).

**L'Overlap (Chevauchement)** : On garde toujours 10-20% du chunk pr√©c√©dent pour ne pas perdre le contexte √† la fronti√®re de coupure.

### C. Embedding & Indexing
On passe chaque chunk dans le mod√®le d'embedding (ex: `text-embedding-3-small`) et on stocke le vecteur dans la Vector DB.

---

## 3. Le Pipeline de Retrieval (R√©cup√©ration)

Quand l'utilisateur pose une question, comment trouver le *meilleur* chunk ?

### A. Semantic Search (Dense Retrieval)
Utilise la similarit√© cosinus.
*   *Avantage* : Comprend le sens ("Voiture" trouve "Automobile").
*   *Inconv√©nient* : Rate parfois les mots-cl√©s exacts (ex: un num√©ro de s√©rie sp√©cifique "XJ-900").

### B. Keyword Search (Sparse Retrieval - BM25)
Le bon vieux moteur de recherche (type ElasticSearch). Compte la fr√©quence des mots.
*   *Avantage* : Trouve les termes exacts.
*   *Inconv√©nient* : Ne comprend pas les synonymes.

### C. Hybrid Search (Le Best Practice) üèÜ
On combine les deux !
$$Score = \alpha \cdot Score_{Vector} + (1 - \alpha) \cdot Score_{BM25}$$
Cela garantit de trouver les concepts proches ET les r√©f√©rences exactes.

---

## 4. Le Re-Ranking (L'√©tape de finition)

Souvent, la recherche vectorielle ram√®ne 50 documents "√† peu pr√®s" pertinents.
On utilise un **Cross-Encoder (Re-ranker)**. C'est un mod√®le plus lent mais tr√®s pr√©cis (ex: `Cohere Rerank`, `BGE-Reranker`) qui va lire la paire (Question, Document) et donner un score de pertinence pr√©cis.
On garde le Top 5 pour le context window du LLM.

---

## 5. L'Architecture Finale

```mermaid
graph LR
    User[Utilisateur] -->|Question| App
    subgraph Ingestion
        Docs[Documents] --> Chunking
        Chunking --> Embedding[Embedding Model]
        Embedding --> VectorDB[(Vector DB)]
    end
    subgraph Retrieval
        App -->|Query| VectorDB
        VectorDB -->|Top K Chunks| ReRanker
        ReRanker -->|Top 5 Context| LLM
    end
    LLM -->|R√©ponse + Sources| User
```

## 6. Les limites actuelles
*   **"Lost in the Middle"** : Les LLM accordent plus d'importance au d√©but et √† la fin du contexte. Si la r√©ponse est au milieu, ils peuvent la rater.
*   **Contexte Window infini ?** : Gemini 1.5 Pro a 1M tokens. Est-ce la fin du RAG ?
    *   Pas tout √† fait. Chercher dans 1M tokens est lent et co√ªteux (‚Ç¨). Le RAG reste pertinent pour la latence et le co√ªt.

