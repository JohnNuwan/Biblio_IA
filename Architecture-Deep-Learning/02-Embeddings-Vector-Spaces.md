# 02 - Embeddings et Espaces Vectoriels

C'est la pierre angulaire des LLM et du RAG. Comment transformer un mot (symbole) en nombre (mathÃ©matique) tout en gardant son sens ?

---

## 1. Le ProblÃ¨me du Langage pour une Machine

Pour un ordinateur, "Roi" et "Reine" sont deux chaÃ®nes de caractÃ¨res totalement diffÃ©rentes, aussi diffÃ©rentes que "Roi" et "Chaussure".
Il n'y a pas de lien sÃ©mantique *inhÃ©rent*.
L'objectif des embeddings est de crÃ©er une reprÃ©sentation gÃ©omÃ©trique oÃ¹ :
$$Distance(Roi, Reine) \approx Distance(Homme, Femme)$$

---

## 2. Word2Vec : La RÃ©volution (2013)

CrÃ©Ã© par **Tomas Mikolov (Google)**.
L'idÃ©e gÃ©niale : **"On connaÃ®t un mot par les mots qui l'entourent"** (HypothÃ¨se distributionnelle).
Si "Pizza" et "PÃ¢tes" apparaissent souvent Ã  cÃ´tÃ© de "DÃ©licieux", "Manger", "Italien", alors "Pizza" et "PÃ¢tes" doivent avoir des reprÃ©sentations numÃ©riques proches.

### Comment Ã§a marche ?
On entraÃ®ne un rÃ©seau de neurones (superficiel) sur tout WikipÃ©dia pour prÃ©dire le mot masquÃ© au milieu d'une phrase.
*   Phrase : "Le chat mange la [?]"
*   Le rÃ©seau apprend Ã  prÃ©dire "souris", "patÃ©e".
*   On jette le rÃ©seau de prÃ©diction, mais **on garde les poids de la couche cachÃ©**.
*   Ce vecteur de poids EST l'embedding du mot.

---

## 3. Les Espaces Vectoriels (Vector Spaces)

Chaque mot (ou phrase, ou image) devient un point dans un espace Ã  N dimensions (souvent 768, 1536 ou 3072 dimensions pour les modÃ¨les modernes comme OpenAI `text-embedding-3-small`).

### Exemple simplifiÃ© en 2D :
Imaginez un graphique avec l'axe X (Concept : Royal) et l'axe Y (Concept : Genre).
*   **Roi** : [0.9, 0.9] (TrÃ¨s Royal, TrÃ¨s Masculin)
*   **Reine** : [0.9, 0.1] (TrÃ¨s Royal, TrÃ¨s FÃ©minin)
*   **Pomme** : [0.0, 0.5] (Pas Royal, Neutre)

### ğŸ§® L'AlgÃ¨bre des Mots
La cÃ©lÃ¨bre Ã©quation dÃ©montrÃ©e par Word2Vec :
$$Vecteur(Roi) - Vecteur(Homme) + Vecteur(Femme) \approx Vecteur(Reine)$$
Cela prouve que l'espace vectoriel a capturÃ© des relations sÃ©mantiques complexes.

---

## 4. La Recherche de Similitude (Vector Search)

Comment trouver les documents pertinents pour une question user ?
On ne cherche pas les *mots exacts* (keyword search), on cherche la *proximitÃ© gÃ©omÃ©trique* (semantic search).

La mÃ©trique reine est la **Cosinus SimilaritÃ© (Cosine Similarity)**.
Elle mesure l'angle entre deux vecteurs.
$$Cosine(\vec{A}, \vec{B}) = \frac{\vec{A} \cdot \vec{B}}{||\vec{A}|| \cdot ||\vec{B}||}$$

*   **1** : Les vecteurs pointent dans la mÃªme direction (Synonymes parfaits).
*   **0** : Les vecteurs sont orthogonaux (Aucun rapport, ex: "Salade" et "TurborÃ©acteur").
*   **-1** : OpposÃ©s.

---

## 5. Les Bases de DonnÃ©es Vectorielles (Vector Databases)

Stocker des millions de vecteurs et rechercher le "Plus Proche Voisin" (Nearest Neighbor - KNN) est trÃ¨s coÃ»teux en calcul brute force.
On utilise des algorithmes d'indexation approximative (**ANN - Approximate Nearest Neighbor**) comme **HNSW (Hierarchical Navigable Small World)**.

### ğŸ¢ Outils du MarchÃ©
*   **Pinecone** (SaaS, trÃ¨s populaire).
*   **ChromaDB** (Open source, local).
*   **Qdrant** (Performance Rust).
*   **pgvector** (Extension pour PostgreSQL - trÃ¨s utilisÃ©e pour ne pas changer de stack technique).

---

## RÃ©sumÃ© du Flux
1.  **User** : "J'ai mal Ã  la tÃªte".
2.  **Embedding Model** : Transforme la phrase en vecteur `[0.12, -0.98, ...]` (1536 chiffres).
3.  **Vector DB** : Cherche dans l'index les vecteurs les plus proches (Cosine Similarity > 0.8).
4.  **Result** : Trouve des docs sur "Migraine", "CÃ©phalÃ©e", "Aspirine" (mÃªme sans le mot "tÃªte" !).
