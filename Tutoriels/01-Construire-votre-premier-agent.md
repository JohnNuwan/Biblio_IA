# üèóÔ∏è Tutoriel 1 : Construire votre Premier Agent IA (Python)

Ce tutoriel vous guide pas-√†-pas pour coder un Agent IA simple capable d'utiliser des outils, sans utiliser de framework complexe (comme LangChain) au d√©but, pour bien comprendre les concepts.

## üéØ Objectif
Cr√©er un script Python qui permet √† un LLM (GPT-4 ou GPT-3.5) de :
1. Comprendre une demande utilisateur.
2. D√©cider d'utiliser un outil (ex: obtenir la m√©t√©o).
3. Ex√©cuter l'outil.
4. R√©pondre √† l'utilisateur avec le r√©sultat.

## üõ†Ô∏è Pr√©-requis
- Python install√©.
- Une cl√© API OpenAI (ou un LLM local compatible via Ollama/vLLM).

## √âtape 1 : Le "Cerveau" (Appel API simple)

Tout commence par une fonction pour parler au LLM.

```python
import os
from openai import OpenAI
import json

# Configuration
client = OpenAI(api_key="votre-api-key")  # ou os.getenv("OPENAI_API_KEY")

def ask_llm(messages):
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
        temperature=0
    )
    return response.choices[0].message.content
```

## √âtape 2 : D√©finir les Outils

Un agent a besoin d'outils. Cr√©ons une fonction "dummy" pour simuler une action r√©elle.

```python
def get_weather(location):
    """Donne la m√©t√©o simul√©e pour une ville donn√©e."""
    if "paris" in location.lower():
        return "20¬∞C, Ensoleill√©"
    elif "london" in location.lower():
        return "15¬∞C, Pluvieux"
    else:
        return "Temp√©rature inconnue pour " + location

# Registre des outils disponibles pour notre code Python
tools_map = {
    "get_weather": get_weather
}
```

## √âtape 3 : Le Prompt Syst√®me (Le Coeur de l'Agent)

C'est ici que la magie op√®re. Nous devons expliquer au LLM **comment** utiliser les outils. Nous allons utiliser le format **ReAct** (Reason + Act) simplifi√©.

```python
SYSTEM_PROMPT = """
Tu es un Assistant IA intelligent dot√© d'outils.

Tes outils disponibles :
- get_weather(location: str): Donne la m√©t√©o pour une ville.

Pour r√©pondre √† une question, tu dois suivre ce format STRICTEMENT :

Thought: Je dois r√©fl√©chir √† ce que je dois faire.
Action: NomDeLAction(argument)
OBSERVATION: R√©sultat de l'action
... (r√©p√®te Thought/Action/OBSERVATION si n√©cessaire)
Final Answer: La r√©ponse finale √† l'utilisateur.

Si tu connais d√©j√† la r√©ponse ou si tu n'as pas besoin d'outils :
Thought: Je peux r√©pondre directement.
Final Answer: Ta r√©ponse.
"""
```

## √âtape 4 : La Boucle d'Ex√©cution (The Loop)

L'agent doit boucler : il r√©fl√©chit -> agit -> observe -> recommence jusqu'√† la r√©ponse finale.

```python
import re

def run_agent(user_query):
    # 1. Initialiser la m√©moire
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_query}
    ]
    
    print(f"ü§ñ User: {user_query}")
    
    # Limite pour √©viter les boucles infinies
    for _ in range(5):
        # 2. Demander au LLM
        llm_response = ask_llm(messages)
        print(f"üß† LLM: {llm_response}")
        
        # Ajouter la r√©ponse du LLM √† l'historique
        messages.append({"role": "assistant", "content": llm_response})
        
        # 3. D√©tecter si le LLM veut agir (Action: ...)
        # Regex pour capturer: Action: tool_name(args)
        action_match = re.search(r"Action: (\w+)\((.*)\)", llm_response)
        
        if action_match:
            tool_name = action_match.group(1)
            tool_args = action_match.group(2)
            
            # 4. Ex√©cuter l'outil
            if tool_name in tools_map:
                print(f"üîß Tool Call: {tool_name} with {tool_args}")
                tool_result = tools_map[tool_name](tool_args)
                
                # 5. Donner le r√©sultat au LLM (OBSERVATION)
                observation = f"OBSERVATION: {tool_result}"
                print(f"üëÄ {observation}")
                messages.append({"role": "user", "content": observation})
            else:
                messages.append({"role": "user", "content": f"OBSERVATION: Erreur, outil {tool_name} inconnu."})
        
        elif "Final Answer:" in llm_response:
            # Le LLM a fini !
            final_answer = llm_response.split("Final Answer:")[-1].strip()
            return final_answer
            
    return "J'ai atteint ma limite d'√©tapes sans trouver de r√©ponse."

# Test
print("-" * 50)
reponse = run_agent("Quelle est la m√©t√©o √† Paris ?")
print(f"üì¢ R√©sultat final : {reponse}")
```

## üîç Analyse de ce qui s'est pass√©

1. **User** : "Quelle est la m√©t√©o √† Paris ?"
2. **LLM** (gr√¢ce au Prompt Syst√®me) : "Thought: Je dois v√©rifier la m√©t√©o. Action: get_weather(Paris)"
3. **Python** (Regex) : D√©tecte `get_weather` et ex√©cute la fonction.
4. **Python** : Renvoie "OBSERVATION: 20¬∞C, Ensoleill√©" dans l'historique de conversation.
5. **LLM** : Re√ßoit l'observation. "Thought: J'ai l'info. Final Answer: Il fait 20¬∞C et ensoleill√© √† Paris."

## ‚úÖ Conclusion

Bravo ! Vous avez cod√© un agent "ReAct" (Reasoning + Acting) manuel.
Les frameworks comme **LangChain** ou **CrewAI** automatisent cette boucle (le parsing, la gestion des erreurs, l'historique), mais la logique reste exactement la m√™me.
