# üóÉÔ∏è Tutoriel 6 : Dompter le JSON (Sorties Structur√©es)

Le cauchemar du d√©veloppeur IA : demander du JSON et recevoir du Markdown ou du texte bla-bla autour.
> "Voici le JSON que tu as demand√© : ```json { ... } ```" ‚ùå

Pour int√©grer une IA dans une app, on veut du JSON **pur** et **valid√©**.

## M√©thode 1 : JSON Mode (Le basique)

OpenAI permet de forcer le mode JSON.

```python
from openai import OpenAI
import json

client = OpenAI()

response = client.chat.completions.create(
  model="gpt-3.5-turbo-0125",
  response_format={ "type": "json_object" }, # <--- La cl√© magique
  messages=[
    {"role": "system", "content": "Tu es un extracteur de donn√©es. Tu DOIS r√©pondre en JSON."},
    {"role": "user", "content": "Jean Dupont habite √† Paris et a 30 ans."}
  ]
)

data = json.loads(response.choices[0].message.content)
print(data) 
# {'nom': 'Jean Dupont', 'ville': 'Paris', 'age': 30} (Les cl√©s peuvent varier si non sp√©cifi√©es)
```

**Inconv√©nient** : Le LLM choisit lui-m√™me les noms des cl√©s (`nom` vs `name` vs `full_name`).

## M√©thode 2 : Function Calling (Le Standard Pro)

C'est la m√©thode la plus robuste. On "ment" au LLM en lui disant qu'on a une fonction, et il va g√©n√©rer les arguments pour l'appeler. Ces arguments sont toujours du JSON strict.

Nous allons utiliser **Pydantic** pour d√©finir notre sch√©ma (c'est plus propre).

```python
from openai import OpenAI
import json
from pydantic import BaseModel, Field

# 1. D√©finir la structure attendue
class UserProfile(BaseModel):
    name: str = Field(description="Nom complet de la personne")
    city: str = Field(description="Ville de r√©sidence")
    age: int = Field(description="√Çge de la personne")
    skills: list[str] = Field(description="Liste des comp√©tences techniques trouv√©es")

# 2. Convertir en sch√©ma JSON pour OpenAI
tools = [
    {
        "type": "function",
        "function": {
            "name": "extract_profile",
            "description": "Extrait les infos d'un profil utilisateur",
            "parameters": UserProfile.model_json_schema() # Pydantic fait le travail dur !
        }
    }
]

client = OpenAI()

prompt = "Je m'appelle Alice, j'ai 28 ans, je suis dev Python expert et je vis √† Lyon."

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": prompt}],
    tools=tools,
    tool_choice={"type": "function", "function": {"name": "extract_profile"}} # On FORCE l'appel
)

# 3. R√©cup√©rer et valider le JSON
tool_args = response.choices[0].message.tool_calls[0].function.arguments
data_json = json.loads(tool_args)

# Validation Pydantic (Double s√©curit√©)
profile = UserProfile(**data_json)

print(f"Nom : {profile.name}")
print(f"Comp√©tences : {profile.skills}")
# Sortie garantie : Nom : Alice, Comp√©tences : ['Python']
```

## M√©thode 3 : Structured Outputs (Nouveau 2024)

OpenAI a r√©cemment sorti une m√©thode encore plus simple pour certains mod√®les r√©cents (`gpt-4o-2024-08-06`).

```python
class Step(BaseModel):
    explanation: str
    output: str

class MathReasoning(BaseModel):
    steps: list[Step]
    final_answer: str

completion = client.beta.chat.completions.parse(
    model="gpt-4o-2024-08-06",
    messages=[
        {"role": "system", "content": "Tu es un tuteur de maths."},
        {"role": "user", "content": "Combien font 8x3 + 2 ?"},
    ],
    response_format=MathReasoning, # On passe directement la classe Pydantic !
)

math_response = completion.choices[0].message.parsed
print(math_response.final_answer)
```

## R√©sum√©

| M√©thode | Usage | Fiabilit√© |
|---------|-------|-----------|
| **Prompt seul** | Prototypage rapide | ‚ö†Ô∏è Faible |
| **JSON Mode** | Si le sch√©ma est flexible | ‚úÖ Moyenne |
| **Function Calling** | Production (compatibilit√© max) | üõ°Ô∏è Haute |
| **Structured Outputs** | Production (mod√®les r√©cents) | üõ°Ô∏èüõ°Ô∏è Maximale |
