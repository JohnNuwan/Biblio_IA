# 07 - Local LLM : Faire tourner l'IA sur votre PC avec Ollama

![Local AI Privacy Shield](..\assets\images\local-ai.png)

Pourquoi payer des API et envoyer vos donn√©es confidentielles dans le Cloud quand vous pouvez avoir un "GPT-4 Light" (Llama 3, Mistral) qui tourne directement sur votre ordinateur, gratuitement et hors-ligne ?

---

## 1. Pourquoi le Local ?

| Avantage | Description |
| :--- | :--- |
| **üîí Confidentialit√©** | Vos donn√©es ne quittent jamais votre machine. Id√©al pour donn√©es m√©dicales, juridiques ou code source propri√©taire. |
| **üí∏ Gratuit√©** | Z√©ro co√ªt par token. Vous ne payez que l'√©lectricit√©. |
| **‚ö° Latence** | Pas d'appel r√©seau. La vitesse d√©pend uniquement de votre carte graphique (GPU). |
| **üîß Libert√©** | Aucune censure (sur certains mod√®les) et contr√¥le total des param√®tres. |

---

## 2. L'Outil Roi : Ollama

Ollama est le "Docker des LLMs". C'est un outil en ligne de commande ultra-simple pour Linux, Mac et Windows.

### Installation
Allez sur [ollama.com](https://ollama.com) et t√©l√©chargez l'installateur.

### Lancer un mod√®le
Ouvrez votre terminal (PowerShell ou Cmd) et tapez :

```bash
ollama run llama3
```

C'est tout. Ollama va t√©l√©charger le mod√®le (environ 4 Go) et lancer un chat interactif.

---

## 3. Les Mod√®les √† connaitre

Ne cherchez pas "GPT-4" ici, ce sont des mod√®les "Open Weights".

*   **Llama 3 (8B)** : Le champion de Meta. Rapide, intelligent, polyvalent. Le standard actuel.
*   **Mistral (7B)** : La fiert√© fran√ßaise. Tr√®s performant et efficace.
*   **Qwen 2.5 Coder** : Sp√©cialis√© pour le code. Incroyable pour un petit mod√®le.
*   **Gemma 2 (9B)** : Le mod√®le ouvert de Google. Excellente capacit√© de raisonnement.

---

## 4. Utilisation Avanc√©e (Python)

Ollama n'est pas juste un Chatbot, c'est un serveur API local. Vous pouvez l'interroger avec Python.

```python
import requests
import json

url = "http://localhost:11434/api/generate"

data = {
    "model": "llama3",
    "prompt": "Explique-moi le polymorphisme en Python en une phrase courte.",
    "stream": False
}

response = requests.post(url, json=data)
print(response.json()['response'])
```

### Int√©gration dans VS Code
Installez l'extension **"Continue"** dans VS Code. Configurez-la pour utiliser "Ollama" comme provider.
Boum üí• ! Vous avez un Copilot gratuit et priv√© qui connait votre code.

---

## 5. Mat√©riel Requis (Hardware)

*   **RAM** : 8 Go minimum (pour les mod√®les 7B/8B). 16 Go est confortable.
*   **GPU** : NVIDIA recommand√© (RTX 3060 ou plus) pour une r√©ponse instantan√©e.
*   **CPU** : √áa marche aussi sur processeur (CPU), mais ce sera plus lent (quelques mots par seconde).

> **Conseil :** Commencez par le mod√®le `phi3` de Microsoft (3.8 Go) si vous avez une petite configuration. Il est surprenant.
