# üìö Tutoriel 4 : Discuter avec vos Documents (RAG)

Le **RAG** (Retrieval-Augmented Generation) est la technique reine pour connecter un LLM √† vos propres donn√©es priv√©es (PDFs, Excels, Notion, etc.) sans le r√©-entra√Æner.

## Le Concept en 3 √âtapes

1.  **Indexation** : D√©couper vos documents en morceaux ("chunks") et les transformer en vecteurs num√©riques ("embeddings").
2.  **Recherche (Retrieval)** : Quand l'utilisateur pose une question, on cherche les morceaux les plus similaires math√©matiquement.
3.  **G√©n√©ration** : On envoie la question + les morceaux trouv√©s au LLM pour qu'il r√©ponde.

## Pr√©-requis

Nous allons utiliser `chromadb` (base de vecteurs locale) et `sentence-transformers` (pour les embeddings) car ils sont gratuits et locaux.

```bash
pip install chromadb sentence-transformers openai
```

## Le Code Complet (Python)

Cr√©ez un fichier `simple_rag.py`.

```python
import chromadb
from sentence_transformers import SentenceTransformer
from openai import OpenAI
import os

# 1. Configuration
client = chromadb.Client()
collection = client.create_collection("mes_documents")
embedder = SentenceTransformer('all-MiniLM-L6-v2') # Petit mod√®le rapide et gratuit
llm_client = OpenAI(api_key="votre-api-key")

# Vos documents "priv√©s" (Simul√©s ici)
documents = [
    {"id": "doc1", "text": "Le projet Alpha a un budget de 50k‚Ç¨. Le chef de projet est Alice."},
    {"id": "doc2", "text": "Le projet Beta est en retard. Il manque 3 d√©veloppeurs Python."},
    {"id": "doc3", "text": "La politique de t√©l√©travail autorise 2 jours par semaine (Mardi et Jeudi)."}
]

print("üíæ Indexation des documents en cours...")

# 2. Indexation (Transformation en vecteurs)
for doc in documents:
    # On calcule le vecteur (embedding) du texte
    embedding = embedder.encode(doc['text']).tolist()
    
    # On stocke dans la base Chroma
    collection.add(
        ids=[doc['id']],
        embeddings=[embedding],
        documents=[doc['text']]
    )

print("‚úÖ Indexation termin√©e !")

# 3. Fonction de Chat RAG
def rag_chat(question):
    print(f"\n‚ùì Question : {question}")
    
    # √âtape A : Recherche s√©mantique
    query_embedding = embedder.encode(question).tolist()
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=1 # On prend juste le document le plus pertinent
    )
    
    retrieved_doc = results['documents'][0][0]
    print(f"üìñ Document trouv√© : '{retrieved_doc}'")
    
    # √âtape B : G√©n√©ration avec le contexte
    prompt = f"""
    Tu es un assistant utile. Utilise UNIQUEMENT le contexte ci-dessous pour r√©pondre √† la question.
    
    CONTEXTE :
    {retrieved_doc}
    
    QUESTION :
    {question}
    """
    
    response = llm_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    
    return response.choices[0].message.content

# 4. Test
reponse1 = rag_chat("Qui g√®re le projet Alpha ?")
print(f"ü§ñ R√©ponse : {reponse1}")

reponse2 = rag_chat("Je peux t√©l√©travailler le mercredi ?")
print(f"ü§ñ R√©ponse : {reponse2}")
```

## Pourquoi c'est magique ?

Regardez la deuxi√®me question : *"Je peux t√©l√©travailler le mercredi ?"*.
Le mot "mercredi" n'est **pas** dans le document 3 (qui dit "Mardi et Jeudi").
Pourtant, le RAG va trouver le document sur le t√©l√©travail car s√©mantiquement, c'est proche. Et le LLM va d√©duire que Mercredi n'est pas Mardi ni Jeudi, donc la r√©ponse sera "Non".

## Aller plus loin

Pour un syst√®me de production :
1. Utilisez **LangChain** pour d√©couper vos PDFs (`RecursiveCharacterTextSplitter`).
2. Utilisez une vraie base vectorielle persistante (ChromaDB en mode persistant, Pinecone, Qdrant).
3. Utilisez le mod√®le d'embedding `text-embedding-3-small` d'OpenAI pour une meilleure qualit√© (payant mais peu cher).
