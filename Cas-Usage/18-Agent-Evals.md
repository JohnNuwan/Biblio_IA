# üß™ Beyond Vibe Checks: Measuring Whether Your Agent Really Works with Evals

## üåç Contexte & Enjeux
"J'ai test√© 3 questions, √ßa a l'air de marcher." -> C'est le **Vibe Check**.
Le probl√®me : quand vous changez le prompt pour corriger un bug, vous en cr√©ez souvent deux autres invisibles (R√©gression).
L'enjeu industriel est de passer du "Vibe Check" manuel aux **Evals Automatis√©es** (Unit Tests pour LLM). Sans Evals, vous ne pouvez pas optimiser ou changer de mod√®le en confiance.

---

## 3 Niveaux de "LLM Evaluation"

1.  **Metric-Based (Rouge/BLEU)** :
    *   Compare les mots exacts.
    *   *Limite* : Si l'IA dit "Joyeux" au lieu de "Heureux", le score chute alors que le sens est bon.

2.  **Model-Based Evaluation (LLM-as-a-Judge)** :
    *   Utiliser GPT-4 pour noter la r√©ponse de GPT-3.5.
    *   *Prompt* : "Note la pertinence de cette r√©ponse sur 10 par rapport √† la question".
    *   C'est le standard actuel.

3.  **Human Feedback (RLHF)** :
    *   Le "Gold Standard". Des humains notent A vs B. Lent et cher, mais indispensable pour calibrer le "Juge IA" (voir point 2).

---

## üè¢ Ils l'utilisent d√©j√†

*   **OpenAI** : Utilise une arm√©e de "Evals" internes. Chaque nouvelle version de GPT-4 doit passer des milliers de tests (Maths, Coding, Safety) avant d'√™tre rel√¢ch√©e.
*   **LangSmith (LangChain)** : Plateforme qui permet aux devs de cr√©er des datasets de test ("Golden Datasets") et de lancer des r√©gressions √† chaque commit.
*   **Discord** : Pour leur Clyde AI, ils mesurent non seulement la r√©ponse, mais la "Latence Per√ßue" et le taux d'abandon, corr√©lant la qualit√© technique au business.

## Conclusion
"If you can't measure it, you can't improve it."
Tant que vous n'avez pas d'Evals, vous ne faites pas de l'ing√©nierie, vous faites de l'alchimie.
