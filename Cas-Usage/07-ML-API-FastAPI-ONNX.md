# üõ†Ô∏è Cr√©er des APIs ML Robustes : FastAPI + ONNX

## üåç Contexte & Enjeux
La mise en production (MLOps) est souvent le goulot d'√©tranglement des projets d'IA. Un mod√®le qui tourne dans un Notebook Jupyter n'est pas un produit. L'enjeu est de servir des pr√©dictions √† haute fr√©quence, avec une faible latence et sans exploser les co√ªts d'infrastructure. Le standard actuel d√©laisse les serveurs lourds Flask/PyTorch au profit de microservices l√©gers et asynchrones, optimis√©s pour l'inf√©rence plut√¥t que l'entra√Ænement.

D√©ployer un mod√®le de Machine Learning en production est souvent plus dur que de l'entra√Æner.
Le standard de l'industrie en 2025 pour les microservices ML est le duo : **FastAPI** (Web) + **ONNX** (Inf√©rence Universelle).

---

## Pourquoi ce duo ?

1.  **FastAPI** : C'est le framework Python le plus rapide. Il g√®re l'asynchrone nativement (utile pour les mod√®les qui bloquent le CPU) et g√©n√®re la documentation Swagger automatiquement.
2.  **ONNX (Open Neural Network Exchange)** : C'est un format de fichier standard.
    *   Vous entra√Ænez en PyTorch (`.pt`).
    *   Vous exportez en ONNX (`.onnx`).
    *   **Avantaqe** : ONNX Runtime est souvent **2x √† 10x plus rapide** que PyTorch pur pour l'inf√©rence CPU. Plus besoin de d√©pendances lourdes (PyTorch p√®se 800Mo, ONNX Runtime p√®se 10Mo).

---

## Le Code (Tutoriel Express)

### 1. Export du Mod√®le (Exemple Scikit-Learn ou PyTorch)

```python
# Exemple fictif d'export
import torch
import torch.onnx

model = MyModel()
dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(model, dummy_input, "model.onnx")
```

### 2. L'API FastAPI

Cr√©ez `api.py` :

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import onnxruntime as ort
import numpy as np

app = FastAPI(title="ML Inference API")

# Charger le mod√®le ONNX au d√©marrage (Une seule fois !)
session = ort.InferenceSession("model.onnx")
input_name = session.get_inputs()[0].name

class PredictionRequest(BaseModel):
    features: list[float]

@app.post("/predict")
async def predict(request: PredictionRequest):
    try:
        # Pr√©traitement
        data = np.array([request.features], dtype=np.float32)
        
        # Inf√©rence ONNX (Fulgurant)
        result = session.run(None, {input_name: data})
        
        # Post-traitement
        prediction = float(result[0][0])
        
        return {"prediction": prediction, "status": "success"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Lancer avec : uvicorn api:app --reload
```

---

## Bonnes Pratiques de Pro

1.  **Batching** : Si vous avez beaucoup de trafic, ne traitez pas les requ√™tes une par une. Utilisez un "Batcher" (comme BentoML) pour grouper 10 requ√™tes et les envoyer au GPU en un coup.
2.  **Docker** : FastAPI + ONNX tient dans un conteneur Docker "Slim" de <200Mb. Facile √† d√©ployer sur Kubernetes ou Cloud Run.
3.  **Versioning** : Nommez vos API `/v1/predict`. Le jour o√π le mod√®le change, cr√©ez `/v2/predict`.

---

## üè¢ Ils l'utilisent d√©j√†

*   **Hugging Face** : Leur "Inference Endpoint" utilise massivement ONNX Runtime pour servir des milliers de mod√®les diff√©rents avec une latence minimale.
*   **Microsoft** : Utilise ONNX dans Windows (Windows ML) pour faire tourner l'IA localement sur votre PC sans vider la batterie (optimisation NPU).
*   **Snapchat** : Utilise des mod√®les optimis√©s pour appliquer des filtres AR en temps r√©el sur mobile (optimisation extr√™me requise).

## Conclusion
Ne mettez jamais un mod√®le `.pickle` ou PyTorch brut en production si vous cherchez la performance. **Convertissez tout en ONNX**.
C'est le "MP3" du Machine Learning : universel, l√©ger, rapide.
