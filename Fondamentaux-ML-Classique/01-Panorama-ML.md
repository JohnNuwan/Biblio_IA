# 01 - Panorama des Types d'Apprentissage Machine (Machine Learning)

![Panorama ML Types](..\assets\images\ml-panorama.png)

Ce guide pose les fondations indispensables pour comprendre l'√©cosyst√®me IA. Avant de parler de "Prompt Engineering" ou de "Transformers", il faut comprendre les paradigmes qui r√©gissent l'apprentissage des machines depuis les ann√©es 1950.

---

## 1. L'Apprentissage Supervis√© (Supervised Learning)

C'est le paradigme le plus r√©pandu et le plus mature industriellement.

### üìú Origine & Histoire
Le concept remonte aux travaux de **Frank Rosenblatt** avec le **Perceptron (1957)**, bien que les m√©thodes statistiques sous-jacentes (comme la r√©gression lin√©aire) datent de **Gauss** et **Legendre** (d√©but XIXe si√®cle). L'id√©e est simple : l'humain enseigne √† la machine par l'exemple.

### üéØ Le Probl√®me R√©solu
Comment automatiser une d√©cision ou une pr√©diction quand on dispose d'un historique de donn√©es fiables (V√©rit√© Terrain / Ground Truth) ?
*   "Voici 10 000 emails, ceux marqu√©s 'spam' et ceux marqu√©s 'non-spam'. Apprends √† classer le 10 001√®me."

### üßÆ Le Concept Math√©matique
On cherche √† approximer une fonction $f$ telle que $Y = f(X) + \epsilon$ (o√π $\epsilon$ est l'erreur irr√©ductible).
On dispose d'un dataset $D = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$ o√π $x$ est l'entr√©e (features) et $y$ la sortie attendue (label).
L'objectif est de minimiser une **Fonction de Co√ªt (Loss Function)**, par exemple l'erreur quadratique moyenne (MSE) :
$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

### üè¢ Qui l'utilise ?
- **Banques (JPMorgan, HSBC)** : Scoring de cr√©dit, d√©tection de fraude.
- **M√©decine (IBM Watson Health)** : Diagnostic √† partir d'imageries (Classification Tumeur b√©nigne/maligne).
- **Email Providers (Gmail)** : Filtrage anti-spam.

---

## 2. L'Apprentissage Non-Supervis√© (Unsupervised Learning)

Ici, la machine est livr√©e √† elle-m√™me face √† des donn√©es brutes, sans √©tiquettes.

### üìú Origine & Histoire
Popularis√© dans les ann√©es 60-70 pour l'analyse exploratoire de donn√©es. Les algorithmes de **Clustering** comme **K-Means** (propos√© par Stuart Lloyd en 1957) sont des piliers.

### üéØ Le Probl√®me R√©solu
Comment trouver une structure cach√©e, des motifs ou des anomalies dans des donn√©es que personne n'a tri√©es ?
*   "Voici les comportements d'achat de 1 million de clients. Regroupe-les par types de profils, sans que je te dise quels sont ces profils."

### üßÆ Le Concept Math√©matique
Il n'y a pas de $y$ (label). On cherche √† mod√©liser la distribution de probabilit√© $P(X)$.
Pour le **Clustering (K-Means)**, on cherche √† minimiser l'inertie intra-classe (la distance entre les points et le centre de leur groupe) :
$$J = \sum_{j=1}^{k} \sum_{i=1}^{n} ||x_i^{(j)} - \mu_j||^2$$
O√π $\mu_j$ est le centre du cluster $j$.

### üè¢ Qui l'utilise ?
- **Netflix / Amazon** : Syst√®mes de recommandation (filtrage collaboratif "Ceux qui ont achet√© A ont aussi achet√© B").
- **Cybers√©curit√© (Darktrace)** : D√©tection d'anomalies r√©seau (d√©tecter un comportement qui s'√©loigne de la "norme" apprise, sans savoir √† quoi ressemble une attaque pr√©cise).
- **Marketing** : Segmentation client automatique.

---

## 3. L'Apprentissage par Renforcement (Reinforcement Learning - RL)

C'est l'apprentissage par l'exp√©rience, inspir√© de la psychologie comportementale.

### üìú Origine & Histoire
Inspir√© par les travaux sur le conditionnement animal (Pavlov, Skinner). Formalis√© math√©matiquement par **Richard Bellman** (ann√©es 50) avec les **Processus de D√©cision Markoviens (MDP)**. A explos√© m√©diatiquement avec **DeepMind** (AlphaGo) en 2016.

### üéØ Le Probl√®me R√©solu
Comment apprendre √† prendre une suite de d√©cisions complexes pour atteindre un but lointain, dans un environnement incertain ?
*   "Apprends √† piloter cet h√©licopt√®re. Si tu te crashes, c'est mal (-1000 points). Si tu restes stable, c'est bien (+1 point)."

### üßÆ Le Concept Math√©matique
Un **Agent** interagit avec un **Environnement**.
- √Ä l'√©tat $s_t$, il choisit une action $a_t$.
- Il re√ßoit une r√©compense $r_{t+1}$ et passe √† l'√©tat $s_{t+1}$.
Il cherche √† maximiser le **Retour cumul√© esp√©r√©** (la somme des r√©compenses futures) :
$$G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}$$
(O√π $\gamma$ est le facteur d'actualisation entre 0 et 1 : pr√©f√®re-t-on le gain imm√©diat ou futur ?).

### üè¢ Qui l'utilise ?
- **Tesla (Autopilot)** : Navigation et d√©cision de conduite.
- **Google (DeepMind)** : Optimisation du refroidissement des Data Centers (40% d'√©conomie d'√©nergie).
- **Finance (Citadel, Two Sigma)** : Trading algorithmique (apprendre √† ex√©cuter des ordres pour maximiser le profit sans perturber le march√©).
- **Robotique (Boston Dynamics)** : Locomotion et √©quilibre des robots.

---

## R√©sum√© Comparatif

| Type | Donn√©es | Objectif | Exemple math√©matique | Analogie Humaine |
| :--- | :--- | :--- | :--- | :--- |
| **Supervis√©** | √âtiquet√©es $(X, y)$ | Pr√©dire $y$ | Minimiser l'erreur $(y - \hat{y})^2$ | Un √©l√®ve apprend avec un prof qui corrige chaque exercice. |
| **Non-Supervis√©** | Brutes $(X)$ | Trouver une structure | Minimiser la distance intra-groupe | Un explorateur d√©couvre une nouvelle langue et essaie d'en d√©duire la grammaire tout seul. |
| **Renforcement** | √âtats/Actions/R√©compenses | Maximiser le gain | √âquation de Bellman | Un b√©b√© apprend √† marcher : il tombe (douleur), se rel√®ve, marche (joie). |
