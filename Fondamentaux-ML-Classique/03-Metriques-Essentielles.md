# 03 - M√©triques Essentielles : Comment noter son mod√®le ?

![Cible Precision Recall](..\assets\images\metrics-target.png)

Cr√©er un mod√®le est facile. Savoir s'il est *bon* et *utile* pour le business est beaucoup plus subtil. "J'ai 99% de pr√©cision" peut √™tre d√©sastreux dans certains contextes.

---

## 1. Le Pi√®ge de l'Accuracy (La Pr√©cision Globale)
L'**Accuracy** est la m√©trique la plus intuitive :
$$Accuracy = \frac{\text{Nombre de pr√©dictions correctes}}{\text{Nombre total de pr√©dictions}}$$

### ‚ö†Ô∏è Le probl√®me (Le Paradoxe de l'Accuracy)
Imaginez un mod√®le de d√©tection de fraude. Dans la r√©alit√©, 99.9% des transactions sont l√©gitimes et 0.1% sont frauduleuses.
Si je cr√©e un mod√®le "idiot" qui dit **toujours** "L√©gitime" (qui ne d√©tecte rien), j'aurai une Accuracy de **99.9%** !
Pourtant, mon mod√®le est inutile (0% des fraudes d√©tect√©es).
**Conclusion** : L'Accuracy est dangereuse sur les jeux de donn√©es d√©s√©quilibr√©s (imbalanced datasets).

---

## 2. La Matrice de Confusion
C'est la carte d'identit√© r√©elle du mod√®le. Elle croise R√©alit√© vs Pr√©diction.

| | **Pr√©dit : Positif (1)** | **Pr√©dit : N√©gatif (0)** |
| :--- | :--- | :--- |
| **R√©alit√© : Positif (1)** | **Vrai Positif (VP)** <br> *(Bravo, d√©tect√© !)* | **Faux N√©gatif (FN)** <br> *(Rat√©, danger !)* |
| **R√©alit√© : N√©gatif (0)** | **Faux Positif (FP)** <br> *(Fausse alerte)* | **Vrai N√©gatif (VN)** <br> *(Correct)* |

---

## 3. Pr√©cision vs Recall (Le grand dilemme)

D√©rivons deux m√©triques cruciales de cette matrice.

### üéØ Precision (La fiabilit√©)
Quand le mod√®le dit "C'est positif", a-t-il raison ?
$$Precision = \frac{VP}{VP + FP}$$

*   **Cas d'usage : Filtre Spam**. On veut une Pr√©cision maximale.
    *   Si un email est class√© "Spam", il doit l'√™tre vraiment.
    *   On d√©teste les Faux Positifs (un mail important qui part en spam). On pr√©f√®re laisser passer quelques pubs (Faux N√©gatifs).

### üîç Recall (Le Rappel / La Sensibilit√©)
De tous les cas positifs r√©els, combien en a-t-on trouv√© ?
$$Recall = \frac{VP}{VP + FN}$$

*   **Cas d'usage : D√©tection de Cancer**. On veut un Recall maximal.
    *   On veut trouver TOUS les malades.
    *   Un Faux N√©gatif (dire √† un malade "tout va bien") est dramatique.
    *   Un Faux Positif (fausse alerte, on refait un test) est acceptable.

### ‚öñÔ∏è F1-Score (Le compromis)
La moyenne harmonique entre les deux. Utile quand on veut un √©quilibre.
$$F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$$

---

## 4. Courbe ROC et AUC
Pour comparer des mod√®les ind√©pendamment du seuil de d√©cision.
L'**AUC (Area Under the Curve)** est un score entre 0.5 (hasard) et 1 (parfait).
*   0.7 - 0.8 : Acceptable
*   0.8 - 0.9 : Excellent
*   0.9+ : Suspect (Overfitting ?)

---

## 5. Overfitting vs Underfitting (Le Bias-Variance Tradeoff)

### Sur-apprentissage (Overfitting) - "L'√©tudiant qui apprend par c≈ìur"
Le mod√®le apprend "le bruit" des donn√©es d'entra√Ænement.
*   **Sympt√¥me** : 99% de r√©ussite sur les donn√©es d'entra√Ænement, mais 60% sur les nouvelles donn√©es (Test).
*   **Solution** : Ajouter plus de donn√©es, simplifier le mod√®le, Regularization (L1/L2), Dropout (en Deep Learning).

### Sous-apprentissage (Underfitting) - "L'√©tudiant qui n'a rien compris"
Le mod√®le est trop simple pour capturer la complexit√© du probl√®me.
*   **Sympt√¥me** : Mauvaise performance partout.
*   **Solution** : Utiliser un mod√®le plus puissant (ex: passer d'une r√©gression lin√©aire √† un r√©seau de neurones).

---

## Conclusion Business
Avant de lancer un projet IA, posez-vous la question :
**"Qu'est-ce qui co√ªte le plus cher √† mon entreprise ? Un Faux Positif ou un Faux N√©gatif ?"**
C'est cette r√©ponse qui dictera le choix math√©matique de la m√©trique √† optimiser.
